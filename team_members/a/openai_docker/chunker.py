from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

def chunk_text(docs, chunk_size=300, overlap=50):
    # RecursiveCharacterTextSplitter ì •ì˜
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=overlap,
        separators=["\n\n\n", "\n\n", "\n", "ã€‚", ". ", " ", ""]  # í•œêµ­ì–´ ì¶”ê°€
    )

    # ì›ë³¸ docsëŠ” [{"text": "...", "project": "...", "file": "...", "page": ...}, ...] í˜•íƒœë¼ê³  ê°€ì •
    langchain_docs = []
    for doc in docs:
        langchain_docs.append(
            Document(
                page_content=doc["text"],
                metadata={
                    "project": doc["project"],
                    "file": doc["file"],
                    "filepath": doc["filepath"],
                    "page": doc["page"]
                }
            )
        )

    # RecursiveCharacterTextSplitterë¡œ ì²­í‚¹
    chunks = splitter.split_documents(langchain_docs)

    # ë°˜í™˜ í˜•ì‹ ë§ì¶”ê¸° (dict ë¦¬ìŠ¤íŠ¸)
    results = []
    chunk_id = 0
    for c in chunks:
        # ğŸ“Œ ì¶”ê°€: ì„¹ì…˜ íƒ€ì… ìë™ ê°ì§€
        section = "general"
        if "ì°¸ê°€ìê²©" in c.page_content[:100]:
            section = "eligibility"
        elif "ì œì¶œì„œë¥˜" in c.page_content[:100]:
            section = "documents"
        elif "í‰ê°€" in c.page_content[:100]:
            section = "evaluation"
        
        results.append({
            "project": c.metadata["project"],
            "file": c.metadata["file"],
            "filepath": c.metadata["filepath"],
            "page": c.metadata["page"],
            "chunk_id": chunk_id,
            "text": c.page_content,
            "length": len(c.page_content),
            "word_count": len(c.page_content.split()),
            "section_type": section,  # ğŸ“Œ ì¶”ê°€
        })
        chunk_id += 1

    return results
