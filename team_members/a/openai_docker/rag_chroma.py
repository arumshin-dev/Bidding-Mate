from loader import load_documents
from chunker import chunk_text
from embedder import get_embeddings
# from chromadb import PersistentClient
# from vectordb import ChromaVectorDB 
from vectordb_chroma import ChromaVectorDB 
from openai import OpenAI 
import os, unicodedata
import hashlib 

def file_hash(path): 
    # NFC ì •ê·œí™” 
    # path = unicodedata.normalize("NFC", path) 
    if not os.path.exists(path): 
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
    with open(path, "rb") as f: 
        return hashlib.md5(f.read()).hexdigest()
    # import unicodedata, os, hashlib 
    # path_nfc = unicodedata.normalize("NFC", path) 
    # path_nfd = unicodedata.normalize("NFD", path) 
    # for candidate in [path_nfc, path_nfd]: 
    #     if os.path.exists(candidate): with open(candidate, "rb") as f: 
    #         return hashlib.md5(f.read()).hexdigest() 
    # raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

def safe_text(text: str) -> str: 
    text = unicodedata.normalize("NFC", text) 
    return text.encode("utf-8", "ignore").decode("utf-8")

class RAGPipeline:
    def __init__(self, persist_dir="chroma_db", collection_name="rag_collection"):
        # ChromaDB ì´ˆê¸°í™”
        # self.client = PersistentClient(path=persist_dir)
        # self.collection = self.client.get_or_create_collection(name="rag_collection")
        # self.db = ChromaVectorDB(persist_dir, collection_name) 
        self.db = ChromaVectorDB(persist_dir="/work/chroma_db", collection_name="rag_collection")
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def build(self, raw_dir="/work/data/raw"):
        print("ğŸ“„ ë¬¸ì„œ ë¡œë”©...")
        docs = load_documents(raw_dir)

        print("âœ‚ï¸ ì²­í‚¹...")
        chunks = chunk_text(docs)

        # print("ğŸ§  ì„ë² ë”© ìƒì„±...")
        # vectors = get_embeddings(chunks)  # âœ… ë²¡í„°ë§Œ ë°˜í™˜
        # DBì— ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ í•´ì‹œ ê°€ì ¸ì˜¤ê¸° 
        existing = self.db.collection.get() 
        existing_hashes = {m.get("file_hash") for m in existing["metadatas"]} 
        print("DEBUG existing_hashes:", existing_hashes) 
        new_chunks = [] 
        new_vectors = [] 
        # DBì— ì´ë¯¸ ì €ì¥ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸° 
        for chunk in chunks: 
             
            h = file_hash(chunk["filepath"]) 
            if h in existing_hashes: 
                continue # ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œ ê±´ë„ˆë›°ê¸° 
            chunk["file_hash"] = h 
            new_chunks.append(chunk) 
        if new_chunks: 
            print("ğŸ§  ì„ë² ë”© ìƒì„±...")
            vectors = get_embeddings(new_chunks) 
            print("ğŸ’¾ ChromaDBì— ì €ì¥...")
            self.db.save_incremental(new_chunks, vectors)
        # print("ğŸ’¾ ChromaDBì— ì €ì¥...")
        # ids = [str(c["chunk_id"]) for c in chunks]
        # texts = [c["text"] for c in chunks]
        # metadatas = [{"project": c["project"], "file": c["file"], "page": c["page"]} for c in chunks]

        # self.collection.add(
        #     ids=ids,
        #     documents=texts,
        #     metadatas=metadatas,
        #     embeddings=vectors
        # )
        # self.db.save(chunks, vectors)
        print("âœ… RAG build complete")

    def query(self, question, embedder_model="text-embedding-3-small", top_k=3, where=None):
        # from openai import OpenAI
        # client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        print("ğŸ” ì§ˆë¬¸ ì„ë² ë”© ìƒì„±...")
        q_vec = self.client.embeddings.create(
            model=embedder_model,
            input=question
        ).data[0].embedding

        print("ğŸ” ChromaDB ê²€ìƒ‰...")
        # results = self.collection.query(
        #     query_embeddings=[q_vec],
        #     n_results=3
        # )

        # context êµ¬ì„±
        # context = "\n\n".join(results["documents"][0])

        # prompt = f"""
        # ì…ì°° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë§¥ë½ì„ ì°¸ê³ í•´ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
        # ë§¥ë½: {context}
        # ì§ˆë¬¸: {question}
        # """
        top_chunks = self.db.search(q_vec, top_k=top_k, where=where)
        print("DEBUG top_chunks:", top_chunks) 
        if not top_chunks: 
            return "ê´€ë ¨ ë§¥ë½ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤." 
        
        context = "\n\n".join([safe_text(c["text"]) for c in top_chunks]) 
        prompt = f"""ì…ì°° ì§€ì› ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì•„ë˜ ë§¥ë½ì„ ì°¸ê³ í•´ ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”. 
        ë§¥ë½: {context} 
        ì§ˆë¬¸: {question}"""
        print("ğŸ¤– ë‹µë³€ ìƒì„±...")
        completion = self.client.chat.completions.create(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = completion.choices[0].message.content
        # ì¶œì²˜ í‘œì‹œ (ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ) 
        sources = "\n".join([f"- {c['project']} ({c['file']}, p.{c['page']})" for c in top_chunks])
        
        return f"{answer}\n\nì¶œì²˜:\n{sources}"
