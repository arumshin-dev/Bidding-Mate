from loader import load_documents
from chunker import chunk_text
from embedder import get_embeddings
from vectordb import VectorDB
from openai import OpenAI
import os
import unicodedata

def safe_text(text: str) -> str:
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
    text = unicodedata.normalize("NFC", text)
    # surrogate ì½”ë“œ ì œê±°
    return text.encode("utf-8", "ignore").decode("utf-8")

class RAGPipeline:
    def __init__(self, db_path="vectordb.pkl"):
        self.db = VectorDB(db_path)
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY")) # ì—¬ê¸°ì„œ ìƒì„±í•˜ëŠ” ê²Œ ì•ˆì „í•¨

    def build(self, raw_dir="../../../data/raw"):
        print("ğŸ“„ pdf ë¬¸ì„œ ë¡œë”©...")
        docs = load_documents(raw_dir)
        # print("ë¬¸ì„œ ê°œìˆ˜:", len(docs))
        # print("docs[-1]", docs[-1])
        # print("ì²« ë¬¸ì„œ ë‚´ìš© ì¼ë¶€:", docs[-1]['text'][:200] if docs else "ë¬¸ì„œ ì—†ìŒ")
        
        print("âœ‚ï¸ Chunking...")
        chunks = chunk_text(docs)
        
        print("ğŸ§  Embedding...")
        print("ğŸ”§ ì„ë² ë”© ìƒì„± ì‹œì‘")
        vectors = get_embeddings(chunks)
        print("ğŸ”§ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        print("ğŸ’¾ Saving to VectorDB...")
        self.db.save(chunks, vectors)
        print("âœ… RAG build complete")

    def query(self, question, top_k=3):
        print("ğŸ” Searching similar chunks...")

        # 1) ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ë¬¸ìì—´ì„ ì§ì ‘ ë„˜ê²¨ì•¼ í•˜ë¯€ë¡œ ë”°ë¡œ ì²˜ë¦¬) 
        q_vec = self.client.embeddings.create( 
            model="text-embedding-3-small", 
            input=safe_text(question )
        ).data[0].embedding

        # 2) ë²¡í„°ë¡œ ê²€ìƒ‰ 
        top_chunks = self.db.search(q_vec, top_k=3)
        print("DEBUG top_chunks:", top_chunks) # êµ¬ì¡° í™•ì¸ìš©
        if not top_chunks: 
            return "ë§¥ë½ì´ ë¶€ì¡±í•´ ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”."
        # 3) í”„ë¡¬í”„íŠ¸ êµ¬ì„± 
        # dictì—ì„œ textë§Œ êº¼ë‚´ì„œ context êµ¬ì„±
        context = "\n\n".join([c["text"] for c in top_chunks])
        # prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜:\n\n{context}\n\nì§ˆë¬¸: {question}"
        prompt = f'''
        ì…ì°°ì§€ì› ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì…ì°°ê³µê³ ì— ëŒ€í•´ ì•„ë˜ì— ì£¼ì–´ì§„ ë§¥ë½ì„ ì´ìš©í•´ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì„¸ìš”. 
        ì£¼ì–´ì§„ ë§¥ë½ìœ¼ë¡œ ë‹µë³€ì´ ì–´ë µë‹¤ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”. ì–µì§€ë¡œ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”.
        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

        ë§¥ë½:
        {context}

        ì§ˆë¬¸:
        {question}
        '''
        print("ğŸ¤– Generating answer...")
        # 4) LLM í˜¸ì¶œ
        completion = self.client.chat.completions.create(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        answer = completion.choices[0].message.content 
        # 4) ì¶œì²˜ í‘œì‹œ (ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ) 
        sources = "\n".join([f"- {c['project']} ({c['file']}, p.{c['page']})" for c in top_chunks])
        
        return f"{answer}\n\nì¶œì²˜:\n{sources}"

'''
python3 - << 'EOF'
from rag import RAGPipeline

rag = RAGPipeline("testdb.pkl")
rag.build("../../../data/raw")

answer = rag.query("ì…ì°° ê³µê³  ì¡°ê±´ ìš”ì•½í•´ì¤˜")
print("ë‹µë³€:", answer)
EOF

'''
