from loader import load_documents
from chunker import chunk_text
from embedder import get_embeddings
from vectordb import VectorDB
from openai import OpenAI

# client = OpenAI()

class RAGPipeline:
    def __init__(self, db_path="vectordb.pkl"):
        self.db = VectorDB(db_path)
        self.client = OpenAI() # ì—¬ê¸°ì„œ ìƒì„±í•˜ëŠ” ê²Œ ì•ˆì „í•¨

    def build(self, raw_dir="../../../data/raw"):
        print("ğŸ“„ pdf ë¬¸ì„œ ë¡œë”©...")
        docs = load_documents(raw_dir)
        print("âœ‚ï¸ Chunking...")
        chunks = chunk_text(docs)
        print("ğŸ§  Embedding...")
        print("ğŸ”§ ì„ë² ë”© ìƒì„± ì‹œì‘")
        vectors = get_embeddings(chunks)
        print("ğŸ”§ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        print("ğŸ’¾ Saving to VectorDB...")
        self.db.save(chunks, vectors)
        print("âœ… RAG build complete")

    def query(self, question):
        print("ğŸ” Searching similar chunks...")
        # 1) ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ 
        q_vec = get_embeddings([question])[0] 
        # 2) ë²¡í„°ë¡œ ê²€ìƒ‰ 
        top_chunks = self.db.search(q_vec, top_k=3)
        # 3) í”„ë¡¬í”„íŠ¸ êµ¬ì„± 
        context = "\n\n".join(top_chunks)
        prompt = f"ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜:\n\n{context}\n\nì§ˆë¬¸: {question}"
        print("ğŸ¤– Generating answer...")
        # 4) LLM í˜¸ì¶œ
        completion = self.client.chat.completions.create(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        return completion.choices[0].message.content

'''
python3 - << 'EOF'
from rag import RAGPipeline

rag = RAGPipeline("testdb.pkl")
rag.build("../../../data/raw")

answer = rag.query("ì…ì°° ê³µê³  ì¡°ê±´ ìš”ì•½í•´ì¤˜")
print("ë‹µë³€:", answer)
EOF

'''
